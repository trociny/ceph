'''
Task that deploys a Ceph cluster using DeepSea
'''
import logging

import deepsea
#from teuthology.exceptions import (CommandFailedError, ConfigError)
#from teuthology.orchestra import run
#from teuthology.misc import (
#    sh,
#    sudo_write_file,
#    write_file,
#    )
#from util import (
#    check_config_key,
#    copy_directory_recursively,
#    sudo_append_to_file,
#    )

log = logging.getLogger(__name__)
cluster_roles = ['mon', 'mgr', 'osd', 'mds', 'rgw', 'igw', 'ganesha']
global_conf = '/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf'
health_ok_cmd = "health-ok.sh --teuthology"
mon_conf = '/srv/salt/ceph/configuration/files/ceph.conf.d/mon.conf'
proposals_dir = "/srv/pillar/ceph/proposals"


def remote_run_script_as_root(remote, path, data):
    """
    Wrapper around write_file to simplify the design pattern:
    1. use write_file to create bash script on the remote
    2. use Remote.run to run that bash script via "sudo bash $SCRIPT"
    """
    write_file(remote, path, data)
    remote.run(label=path, args='sudo bash {}'.format(path))


class Deploy(DeepSea):

    def __init__(self, ctx, config):
        super(Deploy, self).__init__(ctx, config)
        log.debug("beginning of constructor method")
        # deepsea_ctx = ctx['deepsea_ctx']
        self.health_ok_cmd = health_ok_cmd
        if self.config['cli']:
            self.health_ok_cmd += ' --cli'
        deploy_cmdlist = check_config_key(
                         self.config,
                         'commands',
                         [self.health_ok_cmd],
                         )
        if not isinstance(deploy_cmdlist, list) or not deploy_cmdlist:
            raise ConfigError(
                    "deepsea_deploy: commands config param takes a list")
        log.info("deepsea_deploy: deployment command list: {}"
                 .format(deploy_cmdlist))
        # log.debug("ctx.config {}".format(ctx.config))
        log.debug("munged config is {}".format(self.config))
        log.debug("end of constructor method")

    def _ceph_cluster_status(self):
        remote_run_script_as_root(
            self.master_remote,
            'ceph_cluster_status.sh',
            self.scripts.ceph_cluster_status,
            )

    def _ceph_conf_mon_allow_pool_delete(self):
        info_msg = "adjusted ceph.conf to allow pool deletes"
        data = "mon allow pool delete = true\n"
        sudo_append_to_file(
            self.master_remote,
            global_conf,
            data,
            )
        log.info(info_msg)

    def _ceph_conf_dashboard(self):
        info_msg = "adjusted ceph.conf for deployment of dashboard MGR module"
        data = "mgr initial modules = dashboard\n"
        sudo_append_to_file(
            self.master_remote,
            mon_conf,
            data,
            )
        log.info(info_msg)

    def _ceph_conf_small_cluster(self):
        """
        Apply necessary ceph.conf for small clusters
        """
        info_msg = (
            "adjusting ceph.conf for operation with {} storage node(s)"
            .format(self.cluster_nodes)
            )
        data = None
        if self.cluster_nodes == 1:
            data = (
                   "mon pg warn min per osd = 16\n"
                   "osd pool default size = 2\n"
                   "osd crush chooseleaf type = 0 # failure domain == osd\n"
                   )
        elif self.cluster_nodes == 2 or self.cluster_nodes == 3:
            data = (
                   "mon pg warn min per osd = 8\n"
                   "osd pool default size = 2\n"
                   )
        if data:
            log.info(info_msg)
            sudo_append_to_file(
                self.master_remote,
                global_conf,
                data,
                )

    def _ceph_health_test(self):
        self.master_remote.run(args=[
            'sudo',
            'salt-call',
            'wait.until',
            run.Raw('status=HEALTH_OK'),
            'timeout=900',
            'check=1',
            ])

    def _copy_health_ok(self):
        """
        Copy health-ok.sh from teuthology VM to master_remote
        """
        suite_path = self.ctx.config.get('suite_path')
        log.info("suite_path is ->{}<-".format(suite_path))
        sh("ls -l {}".format(suite_path))
        health_ok_path = suite_path + "/deepsea/health-ok"
        sh("test -d " + health_ok_path)
        copy_directory_recursively(
                health_ok_path, self.master_remote, "health-ok")
        self.master_remote.run(args=[
            "pwd",
            run.Raw(";"),
            "ls",
            "-lR",
            "health-ok",
            ])

    def _deploy_ceph(self):
        self._run_commands()

    def _dump_global_conf(self):
        self.master_remote.run(args=[
            'ls',
            '-l',
            global_conf,
            run.Raw(';'),
            'cat',
            global_conf,
            ])

    def _dump_mon_conf(self):
        self.master_remote.run(args=[
            'ls',
            '-l',
            mon_conf,
            run.Raw(';'),
            'cat',
            mon_conf,
            ])

    # FIXME: run on each minion individually, and compare deepsea "roles"
    # with teuthology roles!
    def _pillar_items(self):
        self.master_remote.run(args=[
            'sudo',
            'salt',
            '*',
            'pillar.items',
            ])

    # FIXME: convert this into its own class
    def _policy_cfg(self, config):
        """
        Generate policy.cfg from the results of role introspection
        """
        log.info("deepsea_deploy: WWWW: generating policy.cfg")
        log.debug("deepsea_deploy: roles stanza from job yaml: {}"
                  .format(self.roles))
        if not config:
            config = {}
        check_config_key(config, "profile", "teuthology")
        self._policy_cfg_dump_proposals_dir()
        self._policy_cfg_build_base()
        self._policy_cfg_build_x('mon', required=True)
        self._policy_cfg_build_x('mgr', required=True)
        self._policy_cfg_build_x('mds')
        self._policy_cfg_build_x('rgw')
        self._policy_cfg_build_x('igw')
        self._policy_cfg_build_x('ganesha')
        self._policy_cfg_build_profile(config)
        self._policy_cfg_write()
        self._policy_cfg_cat()
        self._policy_cfg_dump_profile_ymls()

    def _policy_cfg_build_base(self):
        """
        policy.cfg boilerplate
        """
        self.policy_cfg = """# policy.cfg generated by deepsea_deploy.py
# Cluster assignment
cluster-ceph/cluster/*.sls
# Common configuration
config/stack/default/global.yml
config/stack/default/ceph/cluster.yml
# Role assignment - master
role-master/cluster/{master_minion}.sls
# Role assignment - admin
role-admin/cluster/*.sls
""".format(master_minion=self.master_remote.hostname)

    def _policy_cfg_build_profile(self, config):
        """
        Add storage profile to policy.cfg
        """
        profile = config['profile']
        if not isinstance(profile, str):
            raise ConfigError(
                "deepsea_deploy post-Stage 1: "
                "profile config param must be a string"
                )
        if profile == 'teuthology':
            raise ConfigError(
                "deepsea_deploy: teuthology profile not implemented yet"
                )
        elif profile == 'default':
            self.__policy_cfg_build_profile_x('default')
        else:
            ConfigError(
                "deepsea_deploy post-Stage 1: unknown profile ->{}<-"
                .format(profile)
                )

    def __policy_cfg_build_profile_x(self, profile):
        self.profile_ymls_to_dump = []
        no_osd_roles = ("deepsea_deploy: no osd roles configured"
                        ", but at least one of these is required.")
        role_dict = {}
        if 'osd' in self.role_lookup_table:
            role_dict = self.role_lookup_table['osd']
        else:
            raise ConfigError(no_osd_roles)
        log.debug(
            "generating policy.cfg lines for osd profile ->{}<- based on {}"
            .format(profile, role_dict)
            )
        if len(role_dict) == 0:
            raise ConfigError(no_osd_roles)
        osd_remotes = list(set(role_dict.values()))
        for osd_remote in osd_remotes:
            log.debug("{} has one or more osd roles".format(osd_remote))
            self.policy_cfg += """# Storage profile - {remote}
profile-{profile}/cluster/{remote}.sls
""".format(remote=osd_remote, profile=profile)
            ypp = ("profile-{}/stack/default/ceph/minions/{}.yml"
                   .format(profile, osd_remote))
            self.policy_cfg += ypp + "\n"
            self.profile_ymls_to_dump.append(
                "{}/{}".format(proposals_dir, ypp))

    def _policy_cfg_build_x(self, role_type, required=False):
        no_roles_of_type = "no {} roles configured".format(role_type)
        but_required = ", but at least one of these is required."
        role_dict = {}
        if role_type in self.role_lookup_table:
            role_dict = self.role_lookup_table[role_type]
        elif required:
            raise ConfigError(no_roles_of_type + but_required)
        else:
            log.debug(no_roles_of_type)
            return None
        log.debug(
            "generating policy.cfg lines for {} based on {}"
            .format(role_type, role_dict)
            )
        if required:
            if len(role_dict.keys()) < 1:
                raise ConfigError(no_roles_of_type + but_required)
        for role_spec, remote_name in role_dict.iteritems():
            self.policy_cfg += (
                '# Role assignment - {}\n'
                'role-{}/cluster/{}.sls\n'
                ).format(role_spec, role_type, remote_name)

    def _policy_cfg_cat(self):
        """
        Dump the remote policy.cfg file to teuthology log.
        """
        self.master_remote.run(args=[
            'cat',
            proposals_dir + "/policy.cfg"
            ])

    def _policy_cfg_dump_profile_ymls(self):
        """
        Dump profile yml files that have been earmarked for dumping.
        """
        for yml_file in self.profile_ymls_to_dump:
            self.master_remote.run(args=[
                'sudo',
                'cat',
                yml_file,
                ])

    def _policy_cfg_dump_proposals_dir(self):
        """
        Dump the entire proposals directory hierarchy to the teuthology log.
        """
        self.master_remote.run(args=[
            'test',
            '-d',
            proposals_dir,
            run.Raw(';'),
            'ls',
            '-lR',
            proposals_dir + '/',
            ])

    def _policy_cfg_write(self):
        """
        Write policy_cfg to master remote.
        """
        sudo_write_file(
            self.master_remote,
            proposals_dir + "/policy.cfg",
            self.policy_cfg,
            perms="0644",
            owner="salt",
            )
        self.master_remote.run(args=[
            "ls",
            "-l",
            proposals_dir + "/policy.cfg"
            ])

    def _run_command_dict(self, cmd_dict):
        """
        Process commands given in form of dict - example:

            commands:
            - stage0:
                update: true,
                reboot: false
        """
        if len(cmd_dict) != 1:
            raise ConfigError(
                    "deepsea_deploy: command dict must have only one key")
        directive = cmd_dict.keys()[0]
        if directive == "stage0":
            config = cmd_dict['stage0']
            target = self._run_stage_0
        elif directive == "stage1":
            config = cmd_dict['stage1']
            target = self._run_stage_1
        elif directive == "policy_cfg":
            config = cmd_dict['policy_cfg']
            target = self._policy_cfg
        elif directive == "stage2":
            config = cmd_dict['stage2']
            target = self._run_stage_2
        elif directive == "stage3":
            config = cmd_dict['stage3']
            target = self._run_stage_3
        elif directive == "stage4":
            config = cmd_dict['stage4']
            target = self._run_stage_4
        elif directive == "state_orch":
            config = cmd_dict['state_orch']
            target = self._state_orch
        else:
            raise ConfigError(
                "deepsea_deploy: unknown directive ->{}<- in command dict"
                .format(directive))
        target(config)

    def _run_command_str(self, cmd, quiet=False):
        if cmd.startswith('health-ok.sh'):
            cmd = "health-ok/" + cmd
            if self.dev_env:
                cmd = "DEV_ENV=true " + cmd
        if not quiet:
            log.info("WWWW: running command ->{}<-".format(cmd))
        self.master_remote.run(args=[
            'sudo',
            'bash',
            '-c',
            cmd,
            ])

    def _run_commands(self):
        for cmd in self.config['commands']:
            log.debug("deepsea_deploy: considering command {}"
                      .format(cmd))
            if isinstance(cmd, dict):
                self._run_command_dict(cmd)
            elif isinstance(cmd, str):
                self._run_command_str(cmd)
            else:
                raise ConfigError(
                          "deepsea_deploy: command must be either dict or str")

    def _run_stage(self, stage_num):
        """Run a stage. Dump journalctl on error."""
        log.info("deepsea_deploy: WWWW: running Stage {}".format(stage_num))
        cmd_str = None
        if self.config['cli']:
            cmd_str = (
                'timeout 60m deepsea '
                '--log-file=/var/log/salt/deepsea.log '
                '--log-level=debug '
                'stage run ceph.stage.{} --simple-output'
                ).format(stage_num)
        else:
            cmd_str = (
                'timeout 60m salt-run '
                '--no-color state.orch ceph.stage.{}'
                ).format(stage_num)
        if self.dev_env:
            cmd_str = 'DEV_ENV=true ' + cmd_str
        try:
            self._run_command_str(cmd_str, quiet=True)
        except CommandFailedError:
            log.error(
                "WWWW: Stage {} failed. ".format(stage_num)
                + "Here comes journalctl!"
                )
            self.master_remote.run(args=[
                'sudo',
                'journalctl',
                '--all',
                ])
            raise

    def _run_stage_0(self, config):
        """
        Run Stage 0
        """
        if not config:
            config = {}
        check_config_key(config, "update", True)
        check_config_key(config, "reboot", False)
        # FIXME: implement alternative defaults
        if not config['update']:
            remote_run_script_as_root(
                self.master_remote,
                'disable_update_in_stage_0.sh',
                self.scripts.disable_update_in_stage_0
                )
        self._run_stage(0)
        self.sm.all_minions_zypper_status()
        self._salt_api_test()

    def _run_stage_1(self, config):
        """
        Run Stage 1
        """
        if not config:
            config = {}
        self._run_stage(1)

    def _run_stage_2(self, config):
        """
        Run Stage 2
        """
        if not config:
            config = {}
        check_config_key(config, "conf", None)
        self._run_stage(2)
        self._pillar_items()
        self._ceph_conf_small_cluster()
        self._ceph_conf_mon_allow_pool_delete()
        self._ceph_conf_dashboard()
        self._dump_global_conf()
        self._dump_mon_conf()

    def _run_stage_3(self, config):
        """
        Run Stage 3
        """
        if not config:
            config = {}
        self._run_stage(3)
        self._ceph_cluster_status()
        self._ceph_health_test()

    def _run_stage_4(self, config):
        """
        Run Stage 4
        """
        if not config:
            config = {}
        self._run_stage(4)
        self._ceph_cluster_status()
        self._ceph_health_test()

    def _salt_api_test(self):
        remote_run_script_as_root(
            self.master_remote,
            'salt_api_test.sh',
            self.scripts.salt_api_test,
            )

    def _state_orch(self, config, reboot=False):
        """
        Run an orchestration. Optionally survive a reboot of the master node.
        """
        if not config:
            config = {}
        check_config_key(config, "name", None)
        if not config["name"]:
            raise ConfigError(
                "deepsea_deploy: state_orch requires name "
                "of orchestration to run"
                )
        cmd_str = None
        if self.config['cli']:
            cmd_str = (
                'timeout 60m deepsea '
                '--log-file=/var/log/salt/deepsea.log '
                '--log-level=debug '
                'salt-run state.orch {} --simple-output'
                ).format(config["name"])
        else:
            cmd_str = (
                'timeout 60m salt-run --no-color state.orch {}'
                ).format(config["name"])
        self._run_command_str(cmd_str)

    def setup(self):
        super(Deploy, self).setup()
        log.debug("beginning of deepsea_deploy task setup method")
        self._copy_health_ok()
        log.debug("end of deepsea_deploy task setup")

    def begin(self):
        super(Deploy, self).begin()
        log.debug("beginning of deepsea_deploy task begin method")
        self._initialization_sequence()
        self._deploy_ceph()
        log.debug("end of deepsea_deploy task begin method")

    def end(self):
        super(Deploy, self).end()
        log.debug("beginning of deepsea_deploy task end method")
        self.sm.gather_logfile('deepsea.log')
        self.sm.gather_logs('ganesha')
        log.debug("end of deepsea_deploy task end method")

    def teardown(self):
        super(Deploy, self).teardown()
#       log.debug("beginning of deepsea_deploy task teardown method")
        pass
#       log.debug("end of deepsea_deploy task teardown method")
